{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mr/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(dirname):\n",
    "    ids_classes = []\n",
    "    trees = []\n",
    "    for fname in os.listdir(dirname):\n",
    "        if fname == '.DS_Store':\n",
    "            continue\n",
    "        id_str, clazz = fname.split('.')[:2]\n",
    "        ids_classes.append((id_str, clazz))\n",
    "        tree = ET.parse(os.path.join(dirname, fname))\n",
    "        trees.append(tree)\n",
    "    return ids_classes, trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ids_classes, train_trees = read_files('train')\n",
    "test_ids_classes, test_trees = read_files('test')\n",
    "\n",
    "train_df = pd.DataFrame.from_records(train_ids_classes, columns=['id','class']) \n",
    "y_train = train_df['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(tree, small=True):\n",
    "    not_calls = ['processes','all_section','thread','process']\n",
    "    feats = []\n",
    "\n",
    "    for ele in tree.iter():\n",
    "        if ele.tag not in not_calls:\n",
    "            feats.append(ele.tag)\n",
    "            for attr in ['filename', 'srcfile']:\n",
    "                try:\n",
    "                    filename = ele.attrib[attr].split('\\\\')[-1].replace('.','_')\n",
    "                    if filename:                                         \n",
    "                        feats.append(filename)\n",
    "                except:\n",
    "                    pass\n",
    "            if small==False:\n",
    "                for attr in ['desiredaccess','shareaccess','flags','apifunction']:\n",
    "                    try:\n",
    "                         feats.append(ele.attrib[attr].replace('.','_').replace(' ','_'))\n",
    "                    except:\n",
    "                        pass\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features =[]\n",
    "for tree in train_trees:\n",
    "    features = get_features(tree, small=False)\n",
    "    train_features.append(' '.join(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4))\n",
    "vectorizer.fit(train_features)\n",
    "X_train = vectorizer.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_split_mask(n_samples, train_size=0.8):\n",
    "    train, test = train_test_split(range(n_samples), train_size=train_size)\n",
    "    mask = np.ones(n_samples, dtype='int')\n",
    "    mask[train] = 1\n",
    "    mask[test] = 0\n",
    "    mask = (mask==1)\n",
    "    return mask\n",
    "mask = create_split_mask(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_and_score(clf, X, y, mask):\n",
    "    X_train = X[mask]\n",
    "    X_val = X[~mask]\n",
    "    y_train = y[mask]\n",
    "    y_val = y[~mask]\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_preds = clf.predict(X_train)\n",
    "    print 'train accuracy: ' + str(metrics.accuracy_score(y_train, train_preds))\n",
    "    val_preds = clf.predict(X_val)\n",
    "    print 'validation accuracy: ' + str(metrics.accuracy_score(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf ,params, X, y,  n_folds=3):\n",
    "    gs = GridSearchCV(clf, param_grid=params, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    return gs.best_estimator_,gs.best_params_, gs.best_score_, gs.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "validation accuracy: 0.894822006472\n"
     ]
    }
   ],
   "source": [
    "rfc_selector = RFC(n_estimators=100)\n",
    "classify_and_score(rfc_selector, X_train, y_train, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.997974068071\n",
      "validation accuracy: 0.902912621359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "feature_selector = SelectFromModel(rfc_selector, prefit=True, threshold=0.0001)\n",
    "X_trans = feature_selector.transform(X_train)\n",
    "rfc = RFC(n_estimators=100)\n",
    "classify_and_score(rfc, X_trans, y_train, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3086, 1650)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "validation accuracy: 0.906148867314\n"
     ]
    }
   ],
   "source": [
    "feature_selector = SelectFromModel(rfc_selector, prefit=True, threshold=0.00001)\n",
    "X_trans2 = feature_selector.transform(X_train)\n",
    "classify_and_score(rfc, X_trans2, y_train, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9943273906\n",
      "validation accuracy: 0.902912621359\n"
     ]
    }
   ],
   "source": [
    "feature_selector = SelectFromModel(rfc_selector, prefit=True, threshold=0.001)\n",
    "X_trans3 = feature_selector.transform(X_train)\n",
    "classify_and_score(rfc, X_trans3, y_train, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " {'n_estimators': 100},\n",
       " 0.89695398574206087,\n",
       " [mean: 0.89404, std: 0.00592, params: {'n_estimators': 50},\n",
       "  mean: 0.89695, std: 0.00651, params: {'n_estimators': 100},\n",
       "  mean: 0.89598, std: 0.00598, params: {'n_estimators': 250},\n",
       "  mean: 0.89533, std: 0.00727, params: {'n_estimators': 500}])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RFC()\n",
    "params = {'n_estimators':[50, 100, 250, 500]}\n",
    "results = cv_optimize(rfc, params, X_trans, y_train)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.61591312\n",
      "Iteration 2, loss = 2.26723898\n",
      "Iteration 3, loss = 1.82939342\n",
      "Iteration 4, loss = 1.42971106\n",
      "Iteration 5, loss = 1.20282373\n",
      "Iteration 6, loss = 1.08648334\n",
      "Iteration 7, loss = 1.00860293\n",
      "Iteration 8, loss = 0.94909420\n",
      "Iteration 9, loss = 0.90094043\n",
      "Iteration 10, loss = 0.85999568\n",
      "Iteration 11, loss = 0.82476623\n",
      "Iteration 12, loss = 0.79306711\n",
      "Iteration 13, loss = 0.76562616\n",
      "Iteration 14, loss = 0.74058305\n",
      "Iteration 15, loss = 0.71899897\n",
      "Iteration 16, loss = 0.69807836\n",
      "Iteration 17, loss = 0.67913751\n",
      "Iteration 18, loss = 0.66092971\n",
      "Iteration 19, loss = 0.64364995\n",
      "Iteration 20, loss = 0.62805008\n",
      "Iteration 21, loss = 0.61273354\n",
      "Iteration 22, loss = 0.59853544\n",
      "Iteration 23, loss = 0.58523834\n",
      "Iteration 24, loss = 0.57215127\n",
      "Iteration 25, loss = 0.55989105\n",
      "Iteration 26, loss = 0.54814922\n",
      "Iteration 27, loss = 0.53746210\n",
      "Iteration 28, loss = 0.52636727\n",
      "Iteration 29, loss = 0.51627551\n",
      "Iteration 30, loss = 0.50616252\n",
      "Iteration 31, loss = 0.49658510\n",
      "Iteration 32, loss = 0.48766849\n",
      "Iteration 33, loss = 0.47860742\n",
      "Iteration 34, loss = 0.47055645\n",
      "Iteration 35, loss = 0.46273344\n",
      "Iteration 36, loss = 0.45475912\n",
      "Iteration 37, loss = 0.44720794\n",
      "Iteration 38, loss = 0.44026289\n",
      "Iteration 39, loss = 0.43308158\n",
      "Iteration 40, loss = 0.42613677\n",
      "Iteration 41, loss = 0.41992552\n",
      "Iteration 42, loss = 0.41356209\n",
      "Iteration 43, loss = 0.40714125\n",
      "Iteration 44, loss = 0.40099532\n",
      "Iteration 45, loss = 0.39624722\n",
      "Iteration 46, loss = 0.38980040\n",
      "Iteration 47, loss = 0.38440485\n",
      "Iteration 48, loss = 0.37888971\n",
      "Iteration 49, loss = 0.37493318\n",
      "Iteration 50, loss = 0.36892819\n",
      "Iteration 51, loss = 0.36409785\n",
      "Iteration 52, loss = 0.35960747\n",
      "Iteration 53, loss = 0.35467355\n",
      "Iteration 54, loss = 0.35046725\n",
      "Iteration 55, loss = 0.34640019\n",
      "Iteration 56, loss = 0.34155521\n",
      "Iteration 57, loss = 0.33791432\n",
      "Iteration 58, loss = 0.33369244\n",
      "Iteration 59, loss = 0.32944550\n",
      "Iteration 60, loss = 0.32619152\n",
      "Iteration 61, loss = 0.32228455\n",
      "Iteration 62, loss = 0.31864498\n",
      "Iteration 63, loss = 0.31537242\n",
      "Iteration 64, loss = 0.31154187\n",
      "Iteration 65, loss = 0.30852194\n",
      "Iteration 66, loss = 0.30487928\n",
      "Iteration 67, loss = 0.30151258\n",
      "Iteration 68, loss = 0.29846628\n",
      "Iteration 69, loss = 0.29567536\n",
      "Iteration 70, loss = 0.29237377\n",
      "Iteration 71, loss = 0.28942145\n",
      "Iteration 72, loss = 0.28608941\n",
      "Iteration 73, loss = 0.28352109\n",
      "Iteration 74, loss = 0.28042699\n",
      "Iteration 75, loss = 0.27766384\n",
      "Iteration 76, loss = 0.27517275\n",
      "Iteration 77, loss = 0.27261146\n",
      "Iteration 78, loss = 0.27034180\n",
      "Iteration 79, loss = 0.26770564\n",
      "Iteration 80, loss = 0.26529197\n",
      "Iteration 81, loss = 0.26302119\n",
      "Iteration 82, loss = 0.26087707\n",
      "Iteration 83, loss = 0.25854369\n",
      "Iteration 84, loss = 0.25580550\n",
      "Iteration 85, loss = 0.25368105\n",
      "Iteration 86, loss = 0.25143884\n",
      "Iteration 87, loss = 0.24961116\n",
      "Iteration 88, loss = 0.24748631\n",
      "Iteration 89, loss = 0.24528486\n",
      "Iteration 90, loss = 0.24309888\n",
      "Iteration 91, loss = 0.24210824\n",
      "Iteration 92, loss = 0.24004144\n",
      "Iteration 93, loss = 0.23764397\n",
      "Iteration 94, loss = 0.23597544\n",
      "Iteration 95, loss = 0.23360788\n",
      "Iteration 96, loss = 0.23171771\n",
      "Iteration 97, loss = 0.23006848\n",
      "Iteration 98, loss = 0.22883829\n",
      "Iteration 99, loss = 0.22661810\n",
      "Iteration 100, loss = 0.22494867\n",
      "Iteration 101, loss = 0.22320201\n",
      "Iteration 102, loss = 0.22145224\n",
      "Iteration 103, loss = 0.22015495\n",
      "Iteration 104, loss = 0.21872140\n",
      "Iteration 105, loss = 0.21697823\n",
      "Iteration 106, loss = 0.21526689\n",
      "Iteration 107, loss = 0.21350732\n",
      "Iteration 108, loss = 0.21188348\n",
      "Iteration 109, loss = 0.21076805\n",
      "Iteration 110, loss = 0.20942666\n",
      "Iteration 111, loss = 0.20772001\n",
      "Iteration 112, loss = 0.20649539\n",
      "Iteration 113, loss = 0.20461386\n",
      "Iteration 114, loss = 0.20362546\n",
      "Iteration 115, loss = 0.20286125\n",
      "Iteration 116, loss = 0.20137601\n",
      "Iteration 117, loss = 0.19951622\n",
      "Iteration 118, loss = 0.19829876\n",
      "Iteration 119, loss = 0.19697494\n",
      "Iteration 120, loss = 0.19544908\n",
      "Iteration 121, loss = 0.19441236\n",
      "Iteration 122, loss = 0.19320357\n",
      "Iteration 123, loss = 0.19201258\n",
      "Iteration 124, loss = 0.19117393\n",
      "Iteration 125, loss = 0.18961079\n",
      "Iteration 126, loss = 0.18812477\n",
      "Iteration 127, loss = 0.18674437\n",
      "Iteration 128, loss = 0.18569977\n",
      "Iteration 129, loss = 0.18484707\n",
      "Iteration 130, loss = 0.18346822\n",
      "Iteration 131, loss = 0.18201534\n",
      "Iteration 132, loss = 0.18188714\n",
      "Iteration 133, loss = 0.18032180\n",
      "Iteration 134, loss = 0.17935773\n",
      "Iteration 135, loss = 0.17792920\n",
      "Iteration 136, loss = 0.17699378\n",
      "Iteration 137, loss = 0.17687462\n",
      "Iteration 138, loss = 0.17485146\n",
      "Iteration 139, loss = 0.17431020\n",
      "Iteration 140, loss = 0.17279259\n",
      "Iteration 141, loss = 0.17209620\n",
      "Iteration 142, loss = 0.17056525\n",
      "Iteration 143, loss = 0.17031124\n",
      "Iteration 144, loss = 0.16880928\n",
      "Iteration 145, loss = 0.16807784\n",
      "Iteration 146, loss = 0.16719207\n",
      "Iteration 147, loss = 0.16613612\n",
      "Iteration 148, loss = 0.16549714\n",
      "Iteration 149, loss = 0.16439984\n",
      "Iteration 150, loss = 0.16321177\n",
      "Iteration 151, loss = 0.16322815\n",
      "Iteration 152, loss = 0.16179465\n",
      "Iteration 153, loss = 0.16091821\n",
      "Iteration 154, loss = 0.15997543\n",
      "Iteration 155, loss = 0.15884019\n",
      "Iteration 156, loss = 0.15844118\n",
      "Iteration 157, loss = 0.15719971\n",
      "Iteration 158, loss = 0.15734680\n",
      "Iteration 159, loss = 0.15636691\n",
      "Iteration 160, loss = 0.15515346\n",
      "Iteration 161, loss = 0.15456935\n",
      "Iteration 162, loss = 0.15338576\n",
      "Iteration 163, loss = 0.15308711\n",
      "Iteration 164, loss = 0.15217531\n",
      "Iteration 165, loss = 0.15130116\n",
      "Iteration 166, loss = 0.15031978\n",
      "Iteration 167, loss = 0.14975847\n",
      "Iteration 168, loss = 0.14900383\n",
      "Iteration 169, loss = 0.14899634\n",
      "Iteration 170, loss = 0.14690609\n",
      "Iteration 171, loss = 0.14723698\n",
      "Iteration 172, loss = 0.14621215\n",
      "Iteration 173, loss = 0.14518919\n",
      "Iteration 174, loss = 0.14451080\n",
      "Iteration 175, loss = 0.14386997\n",
      "Iteration 176, loss = 0.14364648\n",
      "Iteration 177, loss = 0.14271039\n",
      "Iteration 178, loss = 0.14182314\n",
      "Iteration 179, loss = 0.14125112\n",
      "Iteration 180, loss = 0.14079553\n",
      "Iteration 181, loss = 0.14028311\n",
      "Iteration 182, loss = 0.13932071\n",
      "Iteration 183, loss = 0.13853277\n",
      "Iteration 184, loss = 0.13839096\n",
      "Iteration 185, loss = 0.13789560\n",
      "Iteration 186, loss = 0.13645913\n",
      "Iteration 187, loss = 0.13625772\n",
      "Iteration 188, loss = 0.13640094\n",
      "Iteration 189, loss = 0.13483075\n",
      "Iteration 190, loss = 0.13426582\n",
      "Iteration 191, loss = 0.13366977\n",
      "Iteration 192, loss = 0.13314154\n",
      "Iteration 193, loss = 0.13269339\n",
      "Iteration 194, loss = 0.13205333\n",
      "Iteration 195, loss = 0.13145775\n",
      "Iteration 196, loss = 0.13125361\n",
      "Iteration 197, loss = 0.13070536\n",
      "Iteration 198, loss = 0.12935633\n",
      "Iteration 199, loss = 0.12918760\n",
      "Iteration 200, loss = 0.12883150\n",
      "Iteration 201, loss = 0.12801511\n",
      "Iteration 202, loss = 0.12764344\n",
      "Iteration 203, loss = 0.12675428\n",
      "Iteration 204, loss = 0.12623498\n",
      "Iteration 205, loss = 0.12654416\n",
      "Iteration 206, loss = 0.12567738\n",
      "Iteration 207, loss = 0.12625561\n",
      "Iteration 208, loss = 0.12482579\n",
      "Iteration 209, loss = 0.12454280\n",
      "Iteration 210, loss = 0.12354494\n",
      "Iteration 211, loss = 0.12274886\n",
      "Iteration 212, loss = 0.12288781\n",
      "Iteration 213, loss = 0.12170809\n",
      "Iteration 214, loss = 0.12166554\n",
      "Iteration 215, loss = 0.12099809\n",
      "Iteration 216, loss = 0.12112745\n",
      "Iteration 217, loss = 0.12065056\n",
      "Iteration 218, loss = 0.11960539\n",
      "Iteration 219, loss = 0.11911183\n",
      "Iteration 220, loss = 0.11897418\n",
      "Iteration 221, loss = 0.11798921\n",
      "Iteration 222, loss = 0.11759058\n",
      "Iteration 223, loss = 0.11732296\n",
      "Iteration 224, loss = 0.11755119\n",
      "Iteration 225, loss = 0.11688205\n",
      "Iteration 226, loss = 0.11660991\n",
      "Iteration 227, loss = 0.11552244\n",
      "Iteration 228, loss = 0.11529142\n",
      "Iteration 229, loss = 0.11456742\n",
      "Iteration 230, loss = 0.11455266\n",
      "Iteration 231, loss = 0.11406400\n",
      "Iteration 232, loss = 0.11395760\n",
      "Iteration 233, loss = 0.11312556\n",
      "Iteration 234, loss = 0.11259457\n",
      "Iteration 235, loss = 0.11218472\n",
      "Iteration 236, loss = 0.11186983\n",
      "Iteration 237, loss = 0.11158834\n",
      "Iteration 238, loss = 0.11116519\n",
      "Iteration 239, loss = 0.11080167\n",
      "Iteration 240, loss = 0.11008491\n",
      "Iteration 241, loss = 0.10995244\n",
      "Iteration 242, loss = 0.11027501\n",
      "Iteration 243, loss = 0.10924058\n",
      "Iteration 244, loss = 0.10862412\n",
      "Iteration 245, loss = 0.10830872\n",
      "Iteration 246, loss = 0.10781973\n",
      "Iteration 247, loss = 0.10771407\n",
      "Iteration 248, loss = 0.10741076\n",
      "Iteration 249, loss = 0.10712703\n",
      "Iteration 250, loss = 0.10663678\n",
      "Iteration 251, loss = 0.10683797\n",
      "Iteration 252, loss = 0.10605089\n",
      "Iteration 253, loss = 0.10573241\n",
      "Iteration 254, loss = 0.10498554\n",
      "Iteration 255, loss = 0.10470999\n",
      "Iteration 256, loss = 0.10421407\n",
      "Iteration 257, loss = 0.10417867\n",
      "Iteration 258, loss = 0.10382819\n",
      "Iteration 259, loss = 0.10403067\n",
      "Iteration 260, loss = 0.10363619\n",
      "Iteration 261, loss = 0.10244042\n",
      "Iteration 262, loss = 0.10255202\n",
      "Iteration 263, loss = 0.10212124\n",
      "Iteration 264, loss = 0.10164956\n",
      "Iteration 265, loss = 0.10207862\n",
      "Iteration 266, loss = 0.10115272\n",
      "Iteration 267, loss = 0.10082102\n",
      "Iteration 268, loss = 0.10072926\n",
      "Iteration 269, loss = 0.10045150\n",
      "Iteration 270, loss = 0.10076562\n",
      "Iteration 271, loss = 0.09902173\n",
      "Iteration 272, loss = 0.09963220\n",
      "Iteration 273, loss = 0.09888807\n",
      "Iteration 274, loss = 0.09867862\n",
      "Iteration 275, loss = 0.09840098\n",
      "Iteration 276, loss = 0.09762692\n",
      "Iteration 277, loss = 0.09765192\n",
      "Iteration 278, loss = 0.09737505\n",
      "Iteration 279, loss = 0.09727330\n",
      "Iteration 280, loss = 0.09662529\n",
      "Iteration 281, loss = 0.09679572\n",
      "Iteration 282, loss = 0.09642139\n",
      "Iteration 283, loss = 0.09603540\n",
      "Iteration 284, loss = 0.09529576\n",
      "Iteration 285, loss = 0.09585601\n",
      "Iteration 286, loss = 0.09571275\n",
      "Iteration 287, loss = 0.09483615\n",
      "Iteration 288, loss = 0.09480320\n",
      "Iteration 289, loss = 0.09467611\n",
      "Iteration 290, loss = 0.09406603\n",
      "Iteration 291, loss = 0.09353249\n",
      "Iteration 292, loss = 0.09360099\n",
      "Iteration 293, loss = 0.09296446\n",
      "Iteration 294, loss = 0.09325812\n",
      "Iteration 295, loss = 0.09238174\n",
      "Iteration 296, loss = 0.09305093\n",
      "Iteration 297, loss = 0.09231614\n",
      "Iteration 298, loss = 0.09206010\n",
      "Iteration 299, loss = 0.09222641\n",
      "Iteration 300, loss = 0.09146341\n",
      "Iteration 301, loss = 0.09168216\n",
      "Iteration 302, loss = 0.09100623\n",
      "Iteration 303, loss = 0.09080820\n",
      "Iteration 304, loss = 0.09085070\n",
      "Iteration 305, loss = 0.09042328\n",
      "Iteration 306, loss = 0.08997028\n",
      "Iteration 307, loss = 0.08953777\n",
      "Iteration 308, loss = 0.08961979\n",
      "Iteration 309, loss = 0.08917573\n",
      "Iteration 310, loss = 0.08929439\n",
      "Iteration 311, loss = 0.08899250\n",
      "Iteration 312, loss = 0.08866759\n",
      "Iteration 313, loss = 0.08833556\n",
      "Iteration 314, loss = 0.08819295\n",
      "Iteration 315, loss = 0.08842212\n",
      "Iteration 316, loss = 0.08813463\n",
      "Iteration 317, loss = 0.08759648\n",
      "Iteration 318, loss = 0.08766631\n",
      "Iteration 319, loss = 0.08730355\n",
      "Iteration 320, loss = 0.08675859\n",
      "Iteration 321, loss = 0.08658102\n",
      "Iteration 322, loss = 0.08637148\n",
      "Iteration 323, loss = 0.08594585\n",
      "Iteration 324, loss = 0.08573330\n",
      "Iteration 325, loss = 0.08557267\n",
      "Iteration 326, loss = 0.08549492\n",
      "Iteration 327, loss = 0.08526485\n",
      "Iteration 328, loss = 0.08486036\n",
      "Iteration 329, loss = 0.08481121\n",
      "Iteration 330, loss = 0.08483088\n",
      "Iteration 331, loss = 0.08449757\n",
      "Iteration 332, loss = 0.08421144\n",
      "Iteration 333, loss = 0.08374933\n",
      "Iteration 334, loss = 0.08374211\n",
      "Iteration 335, loss = 0.08340182\n",
      "Iteration 336, loss = 0.08338765\n",
      "Iteration 337, loss = 0.08313776\n",
      "Iteration 338, loss = 0.08314339\n",
      "Iteration 339, loss = 0.08301719\n",
      "Iteration 340, loss = 0.08261649\n",
      "Iteration 341, loss = 0.08249343\n",
      "Iteration 342, loss = 0.08252243\n",
      "Iteration 343, loss = 0.08210632\n",
      "Iteration 344, loss = 0.08275523\n",
      "Iteration 345, loss = 0.08202493\n",
      "Iteration 346, loss = 0.08149066\n",
      "Iteration 347, loss = 0.08123180\n",
      "Iteration 348, loss = 0.08127755\n",
      "Iteration 349, loss = 0.08067455\n",
      "Iteration 350, loss = 0.08093580\n",
      "Iteration 351, loss = 0.08100460\n",
      "Iteration 352, loss = 0.08015932\n",
      "Iteration 353, loss = 0.08051076\n",
      "Iteration 354, loss = 0.08003293\n",
      "Iteration 355, loss = 0.08000958\n",
      "Iteration 356, loss = 0.08031322\n",
      "Iteration 357, loss = 0.07983777\n",
      "Iteration 358, loss = 0.08002934\n",
      "Iteration 359, loss = 0.07902144\n",
      "Iteration 360, loss = 0.07923011\n",
      "Iteration 361, loss = 0.07883228\n",
      "Iteration 362, loss = 0.07852340\n",
      "Iteration 363, loss = 0.07846451\n",
      "Iteration 364, loss = 0.07841266\n",
      "Iteration 365, loss = 0.07807623\n",
      "Iteration 366, loss = 0.07801520\n",
      "Iteration 367, loss = 0.07784930\n",
      "Iteration 368, loss = 0.07758079\n",
      "Iteration 369, loss = 0.07777157\n",
      "Iteration 370, loss = 0.07757576\n",
      "Iteration 371, loss = 0.07703843\n",
      "Iteration 372, loss = 0.07716331\n",
      "Iteration 373, loss = 0.07697656\n",
      "Iteration 374, loss = 0.07732073\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "train accuracy: 0.982576985413\n",
      "validation accuracy: 0.891585760518\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, verbose=True)\n",
    "classify_and_score(mlp, X_trans, y_train, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RFC(n_estimators=100, n_jobs=-1)\n",
    "rfc.fit(X_trans[mask], y_trans[mask])\n",
    "trainpreds_rfc = rfc.predict_proba(X_trans[mask])\n",
    "trainpreds_mlp = mlp.predict_proba(X_trans[mask])\n",
    "valpreds_rfc = rfc.predict_proba(X_trans[~mask])\n",
    "valpreds_mlp = mlp.predict_proba(X_trans[~mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trainpreds = np.concatenate([trainpreds_rfc, trainpreds_mlp], axis=1)\n",
    "stacked = LogisticRegression().fit(X_trainpreds, y_train[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_valpreds = np.concatenate([valpreds_rfc, valpreds_mlp], axis=1)\n",
    "stacked.score(X_valpreds, y_train[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function for writing predictions in the required format\n",
    "def write_predictions(predictions, ids, outfile):\n",
    "    \"\"\"\n",
    "    assumes len(predictions) == len(ids), and that predictions[i] is the\n",
    "    index of the predicted class with the malware_classes list above for \n",
    "    the executable corresponding to ids[i].\n",
    "    outfile will be overwritten\n",
    "    \"\"\"\n",
    "    with open(outfile,\"w+\") as f:\n",
    "        # write header\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i, history_id in enumerate(ids):\n",
    "            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RFC(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', algorithm='adam', alpha=0.0001,\n",
       "       batch_size=200, beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "       epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_rfc = rfc.predict_proba(X_train)\n",
    "preds_mlp = mlp.predict_proba(X_train)\n",
    "X_preds = np.concatenate([preds_rfc, preds_mlp], axis=1)\n",
    "stacked = LogisticRegression().fit(X_preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features =[]\n",
    "for tree in test_trees:\n",
    "    features = get_features(tree, only_calls=True)\n",
    "    test_features.append(' '.join(features) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testpreds_rfc = rfc.predict_proba(X_test)\n",
    "testpreds_mlp = mlp.predict_proba(X_test)\n",
    "X_testpreds = np.concatenate([testpreds_rfc, testpreds_mlp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testpreds = [malware_classes.index(y_hat) for y_hat in stacked.predict(X_testpreds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testids = [id_class[0] for id_class in test_ids_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_predictions(testpreds, testids, 'predictions2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
