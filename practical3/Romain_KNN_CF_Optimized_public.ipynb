{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import boto\n",
    "import pyprind\n",
    "from scipy import stats\n",
    "import scipy\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boto Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = boto.connect_s3(aws_access_key_id='xxxx', \n",
    "                     aws_secret_access_key='xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_bucket_p2 = s3.get_bucket('practicals3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = s3_bucket_p2.new_key('res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv('Data/train.csv')\n",
    "#test = pd.read_csv('Data/test.csv')\n",
    "#train0 = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://s3-us-west-1.amazonaws.com/practicals3/train.csv')\n",
    "test  = pd.read_csv('https://s3-us-west-1.amazonaws.com/practicals3/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = train0.loc[np.in1d(train0.artist, train0.artist.unique()[:20])] # Only use the 20 first artists\n",
    "#train = train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_artist = train.artist.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Compute the baseline\n",
    "1. Compute the Common Support as : $NCommon_{ij}$ How many users rated/played both music\n",
    "2. Compute the Rho_Correlation : $\\rho(\\bf{Y_{u_i}} - \\bf{\\overline{Y_u}} ; \\bf{Y_{u_j}} - \\bf{\\overline{Y_u}})$\n",
    "3. Compute the Similarity : $ \\frac{N_Common \\rho_{mj}}{N_Common + reg}$\n",
    "4. Apply the Central Dogma : $Y_{um} = Y_{um}^{Baseline} + \\frac{\\sum_{j \\in S^k(m)} s_{mj} (Y_{uj} - Y_{um}^{Baseline})}{\\sum_{j \\in S^k(m)} s_{mj}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Approach by filtering given u,m, don't compute all the support etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding IDs ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the speed of the various algorithm, we implement IDs instead of using the text to perform the location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artist_id = pd.DataFrame(columns = ['artist'])\n",
    "artist_id.loc[:, 'artist'] = train.artist.unique()\n",
    "artist_id.loc[:, 'Id'] = artist_id.index\n",
    "artist_id.index = artist_id.artist.values\n",
    "\n",
    "train.loc[:, 'artist_id'] = artist_id.loc[train.artist, 'Id'].values\n",
    "test.loc[:, 'artist_id'] = artist_id.loc[test.artist, 'Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_id = pd.DataFrame(columns = ['user'])\n",
    "user_id.loc[:, 'user'] = train.user.unique()\n",
    "user_id.loc[:, 'Id']   = user_id.index\n",
    "user_id.index = user_id.user.values\n",
    "\n",
    "train.loc[:, 'user_id'] = user_id.loc[train.user, 'Id'].values\n",
    "test.loc[:, 'user_id']  = user_id.loc[test.user, 'Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 : Compute the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# u    = test.loc[0, 'user_id']\n",
    "# m    = test.loc[0, 'artist_id']\n",
    "# u =0\n",
    "# m=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeBaseline(u,m,train):\n",
    "    Y_bar = train.plays.median()\n",
    "    Y_u = train.loc[train.user_id == u].plays.median()\n",
    "    Y_m = train.loc[train.artist_id == m].plays.median()\n",
    "    Y_baseline = Y_bar + Y_u - Y_bar + Y_m - Y_bar\n",
    "    return Y_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#computeBaseline(u,m,train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Get the Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetSupport(u,m,train):\n",
    "\n",
    "    potential_artists = train.loc[train.user_id == u, 'artist_id'].unique()\n",
    "\n",
    "    N_support = np.empty(shape = (len(potential_artists)))\n",
    "    support = [[] for i in range(len(potential_artists))]\n",
    "    prog_bar = pyprind.ProgBar(len(potential_artists))\n",
    "\n",
    "    # Loop on the artist\n",
    "    for j,artist2 in enumerate(potential_artists):\n",
    "        prog_bar.update()\n",
    "        support[j]   = set(train.loc[(train.artist_id == m), 'user_id']).intersection(set(train.loc[(train.artist_id == artist2), 'user_id']))\n",
    "        N_support[j] = len(support[j])\n",
    "        \n",
    "    return (N_support, support, potential_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#N_support, support, base_artist = GetSupport(u,m,train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 : Compute the similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute the user average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prog_bar = pyprind.ProgBar(len(train.user.unique()))\n",
    "# user_avg = pd.DataFrame(columns=['User', 'AVG'])\n",
    "# for u in train.user_id.unique():\n",
    "#     user_avg = user_avg.append({'User': u, 'AVG' : train.loc[train.user_id == u].plays.median()}, ignore_index=True)\n",
    "#     prog_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_avg = pd.read_csv('user_avg2.csv')\n",
    "# user_avg.index = user_avg.User.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarity Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSim(artist1, artist2, unique_artist, support, N_support, train_small, train, reg = 3, verbose=False):\n",
    "    commonUser = support[unique_artist.tolist().index(artist2)]\n",
    "    N_Common = N_support[unique_artist.tolist().index(artist2)]\n",
    "    user_artist1 = []\n",
    "    user_artist2 = []\n",
    "    if(verbose):\n",
    "        prog_bar = pyprind.ProgBar(len(commonUser))\n",
    "    \n",
    "    ix_artist1 = (train.artist_id == artist1)\n",
    "    ix_artist2 = (train.artist_id == artist2)\n",
    "    \n",
    "    for i in commonUser:\n",
    "        #print(i, end = \",\")\n",
    "        if(verbose):\n",
    "            prog_bar.update()\n",
    "        u_avg = train.loc[train.user_id == i].plays.median()\n",
    "        user_artist1.append(int(train_small.loc[(train_small.user_id == i) & ix_artist1].plays) - u_avg)\n",
    "        user_artist2.append(int(train_small.loc[(train_small.user_id == i) & ix_artist2].plays) - u_avg)\n",
    "        \n",
    "    rho = scipy.stats.pearsonr(user_artist1, user_artist2)[0]\n",
    "    rho_shrunk = N_Common * rho / (N_Common + reg) \n",
    "    return((1-rho_shrunk)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sim = np.empty(shape = (len(base_artist)))\n",
    "# artist1 = m\n",
    "# prog_bar = pyprind.ProgBar(len(base_artist))\n",
    "\n",
    "# #train_small\n",
    "# ix = np.in1d(train.artist_id, np.append(m, base_artist))\n",
    "# train_small = train.loc[ix]\n",
    "\n",
    "# for j,artist2 in enumerate(base_artist):\n",
    "#     sim[j] = computeSim(artist1, artist2, base_artist, support, N_support,train_small, train)\n",
    "#     prog_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 : Adjust the Baseline - Get the Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #input:\n",
    "# k = 3\n",
    "\n",
    "# res = pd.DataFrame(columns = ['artist_id', 'sim'])\n",
    "# res.loc[:, 'artist_id'] = base_artist\n",
    "# res.loc[:, 'sim'] = sim\n",
    "# res = res.sort_values(by = 'sim', ascending = False)\n",
    "\n",
    "# # Apply the final Algo\n",
    "# num = 0\n",
    "# denom = 0\n",
    "# Yum_base = computeBaseline(u, m, train)\n",
    "\n",
    "# for i in res.index[:k]:\n",
    "#     Yuj = int(train[(train.user_id == u) & (train.artist_id == res.loc[i, 'artist_id'])].plays)\n",
    "#     num += res.loc[i, 'sim']*(Yuj-Yum_base)\n",
    "#     denom += res.loc[i, 'sim']\n",
    "\n",
    "# Y_um = Yum_base + num/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakePrediction(u,m,train,k=5):\n",
    "    # Compute Support\n",
    "    print('Compute Support')\n",
    "    N_support, support, base_artist = GetSupport(u,m,train)\n",
    "\n",
    "    # Compute the sim\n",
    "    print('Compute Sim')\n",
    "    sim = np.empty(shape = (len(base_artist)))\n",
    "    artist1 = m\n",
    "    prog_bar = pyprind.ProgBar(len(base_artist))\n",
    "\n",
    "    #train_small\n",
    "    ix = np.in1d(train.artist_id, np.append(m, base_artist))\n",
    "    train_small = train.loc[ix]\n",
    "\n",
    "    for j,artist2 in enumerate(base_artist):\n",
    "        sim[j] = computeSim(artist1, artist2, base_artist, support, N_support,train_small, train)\n",
    "        prog_bar.update()\n",
    "        \n",
    "    #input:\n",
    "#    k = 5\n",
    "\n",
    "    res = pd.DataFrame(columns = ['artist_id', 'sim'])\n",
    "    res.loc[:, 'artist_id'] = base_artist\n",
    "    res.loc[:, 'sim'] = sim\n",
    "    res = res.sort_values(by = 'sim', ascending = False)\n",
    "\n",
    "    # Apply the final Algo\n",
    "    print('Apply Final Algo')\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    Yum_base = computeBaseline(u, m, train)\n",
    "\n",
    "    for i in res.index[:k]:\n",
    "        Yuj = int(train[(train.user_id == u) & (train.artist_id == res.loc[i, 'artist_id'])].plays)\n",
    "        num += res.loc[i, 'sim']*(Yuj-Yum_base)\n",
    "        denom += res.loc[i, 'sim']\n",
    "\n",
    "    Y_um = Yum_base + num/denom\n",
    "    \n",
    "    return Y_um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_res = pd.DataFrame(columns = ['Id', 'Prediction'])\n",
    "count = 0\n",
    "\n",
    "for r in test.iterrows():\n",
    "    print('*********' + str(r[1]['Id']) + '*********')\n",
    "    temp = MakePrediction(r[1]['user_id'], r[1]['artist_id'], train)\n",
    "    final_res = final_res.append({'Id':r[1]['Id'], 'Prediction' :temp}, ignore_index = True)\n",
    "    \n",
    "    count += 1\n",
    "    if (count == 100):\n",
    "        print('done')\n",
    "        count = 0\n",
    "        final_res.to_csv('res.csv', index = False)\n",
    "        k.set_contents_from_filename('res.csv')\n",
    "        \n",
    "#print(\"Saving\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
